{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacci Ziebert\n",
    "\n",
    "May 4, 2020\n",
    "\n",
    "Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My analysis was done in 4 parts, each labeled by a number:\n",
    "    1. Python tweet collection and cleaning.\n",
    "    2. R finding matching game titles in tweets with an output of a clean dataframe with game stats from another API\n",
    "    3. R doing data analysis of tweets and board games data. Output is a few charts\n",
    "    4. R writing executive summary, importing graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jsonpickle\n",
    "import tweepy\n",
    "import json\n",
    "import string\n",
    "import csv\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part gathers board game tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1552 tweets\n"
     ]
    }
   ],
   "source": [
    "exec(open('C:/Users/jacci/Documents/DS 710/ds710spring2020finalproject/twitter_credentials.py').read())\n",
    "auth = tweepy.AppAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "\n",
    "#Setting up new api wrapper, using authentication only\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "#Error handling\n",
    "if (not api):\n",
    "    print (\"Problem connecting to API\")\n",
    "\n",
    "# api.rate_limit_status()['resources']['search'] # to check how many queries we have left\n",
    "\n",
    "#This is what we are searching for\n",
    "searchQuery = '-is:retweet -#goldengeekaward -apartments -RT -kickstarter -#kickstarter (#boardgames OR #boardgame OR #gamenight OR #bgstats OR #eurogames OR #boardgaming OR #bggplay)\\\n",
    "                AND (played OR playing OR plays OR tonight OR \"last night\" OR session OR solo)' #1,329\n",
    "\n",
    "maxTweets = 10000 #Maximum number of tweets we want to collect\n",
    "tweetsPerQry = 100 #The twitter Search API allows up to 100 tweets per query\n",
    "tweetCount = 0\n",
    "\n",
    "with open('C:/Users/jacci/Documents/DS 710/ds710spring2020finalproject/boardgames_tweets_test.json', 'w') as f:\n",
    "\n",
    "    #Tell the Cursor method that we want to use the Search API (api.search)\n",
    "    #Also tell Cursor our query, and the maximum number of tweets to return\n",
    "    for tweet in tweepy.Cursor(api.search,q=searchQuery, tweet_mode='extended').items(maxTweets):\n",
    "        #Write the JSON format to the text file, and add one to the number of tweets we've collected\n",
    "        f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "        tweetCount += 1\n",
    "    #Display how many tweets we have collected\n",
    "    print(\"Downloaded {0} tweets\".format(tweetCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I did some cleaning, i.e. removed hashtags, urls, common words, and collected hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C:/Users/jacci/Documents/DS 710/ds710spring2020finalproject/boardgames_tweets4302020.json\"\n",
    "\n",
    "def import_games_json(filename = filename): #import raw tweets and outputs a df with selected columns\n",
    "    with open(filename, 'r', encoding='utf-8') as tweet_data:\n",
    "          games_tweet = [json.loads(line) for line in tweet_data.readlines()]\n",
    "#           game_df = pd.json_normalize(games_tweet)\n",
    "          game_df = json_normalize(games_tweet)\n",
    "    game_df_selected = game_df[['id', 'full_text', 'user.screen_name', 'user.followers_count',\\\n",
    "    'user.friends_count', 'entities.hashtags','retweet_count', 'favorite_count', 'favorited',\\\n",
    "    'retweeted']]\n",
    "    return game_df_selected\n",
    "\n",
    "def return_cleaned_tweet(row): # Remove common words, hashtags, urls\n",
    "    clean_text = row['full_text'].split()\n",
    "    clean_text = [word.lower() for word in clean_text]\n",
    "    clean_text = [word for word in clean_text if word[0] != \"#\" and word[0:4] != \"http\" \\\n",
    "    and word not in (\"played\", \"play\", \"played\", \"playing\" \"i\", \"it's\", \"i'll\",\\\n",
    "     \"tonight\", \"got\", \"marathon\", \"vip\", \"review\", \"podcast\", \"games\", \"today\", \"tonight\")]\n",
    "    # clean_text = [word[:-1] for word in clean_text if word[-1] in string.punctuation]\n",
    "    clean_text = ' '.join(word for word in clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def return_hashtags(row): # Find hashtags\n",
    "    hashtags = row['full_text'].split()\n",
    "    hashtags = [word[1:] for word in hashtags if word[0] == \"#\" and word not in (\"#boardgames\",\\\n",
    "    \"#boardgame\", \"#bggplay\", \"#gamenight\", \"#bgstats\", \"#sologaming\", \"#boardgamegeek\",\\\n",
    "    \"#tabletopgames\",\"#tabletopgaming\", \"#games\", \"#solo\")]\n",
    "    hashtags = ' '.join(word.lower() for word in hashtags)\n",
    "    return hashtags\n",
    "\n",
    "def export_to_csv(df):\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    df.to_csv(\"C:/Users/jacci/Documents/DS 710/ds710spring2020finalproject/cleaned_games_test.csv\")\n",
    "\n",
    "game_tweets = import_games_json()\n",
    "game_tweets['clean.full.text'] = game_tweets.apply(return_cleaned_tweet, axis=1) # new column of cleaned tweet\n",
    "game_tweets['hashtags'] = game_tweets.apply(return_hashtags, axis=1)  # new column of hashtags\n",
    "# take all capitalized and hashtags, returns all lowercase\n",
    "export_to_csv(game_tweets)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None): # i want to print EVERYTHING\n",
    "#     print(game_tweets['clean.full.text'][1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
